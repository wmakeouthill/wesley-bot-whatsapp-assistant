# GLM-4.7 - InstalaÃ§Ã£o e ConfiguraÃ§Ã£o para PC Fraco

Este repositÃ³rio contÃ©m scripts automatizados para instalar e configurar o modelo GLM-4.7 em mÃ¡quinas com recursos limitados.

## ğŸ“‹ Requisitos MÃ­nimos

### Hardware MÃ­nimo (CPU Only)
- **RAM**: 32GB+ (recomendado 64GB+)
- **Disco**: 200GB+ de espaÃ§o livre (SSD recomendado)
- **CPU**: Processador multi-core moderno

### Hardware Recomendado (com GPU)
- **GPU**: 8GB+ VRAM (recomendado 16GB+)
- **RAM**: 64GB+ (recomendado 128GB+)
- **Disco**: 300GB+ de espaÃ§o livre (NVMe SSD recomendado)
- **CUDA**: CompatÃ­vel com CUDA 11.8+ ou 12.1+

## ğŸ¯ VersÃµes de Modelo DisponÃ­veis

O GLM-4.7 estÃ¡ disponÃ­vel em vÃ¡rias quantizaÃ§Ãµes para diferentes capacidades de hardware:

| VersÃ£o | Tamanho | RAM MÃ­nima | VRAM MÃ­nima | Uso Recomendado |
|--------|---------|------------|-------------|-----------------|
| **UD-Q2_K_XL** (2-bit) | ~135GB | 128GB | 24GB | MÃ¡quinas potentes com GPU |
| **Q4_K_M** (4-bit) | ~200GB | 64GB | 16GB | MÃ¡quinas moderadas |
| **Q4_K_S** (4-bit) | ~180GB | 48GB | 12GB | MÃ¡quinas modestas |
| **Q5_K_M** (5-bit) | ~240GB | 80GB | 20GB | Melhor qualidade |

## ğŸš€ InÃ­cio RÃ¡pido

### Windows (PowerShell)

```powershell
# 1. Instalar dependÃªncias
.\scripts\install.ps1

# 2. Baixar modelo (escolha a versÃ£o adequada)
.\scripts\download-model.ps1 -Version "Q4_K_S"

# 3. Executar o modelo
.\scripts\run-llamacpp.ps1
```

### Linux/Mac (Bash)

```bash
# 1. Instalar dependÃªncias
chmod +x scripts/install.sh
./scripts/install.sh

# 2. Baixar modelo (escolha a versÃ£o adequada)
chmod +x scripts/download-model.sh
./scripts/download-model.sh Q4_K_S

# 3. Executar o modelo
chmod +x scripts/run-llamacpp.sh
./scripts/run-llamacpp.sh
```

## ğŸ“ Estrutura do RepositÃ³rio

```
.
â”œâ”€â”€ README.md                 # Este arquivo
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install.sh            # InstalaÃ§Ã£o Linux/Mac
â”‚   â”œâ”€â”€ install.ps1           # InstalaÃ§Ã£o Windows
â”‚   â”œâ”€â”€ download-model.sh     # Download modelo (Linux/Mac)
â”‚   â”œâ”€â”€ download-model.ps1    # Download modelo (Windows)
â”‚   â”œâ”€â”€ run-llamacpp.sh       # Executar com llama.cpp (Linux/Mac)
â”‚   â”œâ”€â”€ run-llamacpp.ps1      # Executar com llama.cpp (Windows)
â”‚   â”œâ”€â”€ run-ollama.sh         # Executar com Ollama (Linux/Mac)
â”‚   â””â”€â”€ run-ollama.ps1        # Executar com Ollama (Windows)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ hardware-config.yaml  # ConfiguraÃ§Ã£o de hardware
â”‚   â””â”€â”€ model-config.json     # ConfiguraÃ§Ãµes do modelo
â””â”€â”€ models/                   # DiretÃ³rio para modelos baixados
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Configurar Hardware

Edite `config/hardware-config.yaml` com as especificaÃ§Ãµes da sua mÃ¡quina:

```yaml
hardware:
  gpu:
    available: true
    vram_gb: 8
    cuda_arch: "75"  # Para RTX 2060, 2070, 2080
  ram_gb: 32
  cpu_cores: 8
  disk_space_gb: 500
```

### 2. Escolher VersÃ£o do Modelo

Baseado no seu hardware, escolha a versÃ£o adequada:

- **PC muito fraco (32GB RAM, sem GPU)**: Use `Q4_K_S` ou considere modelos menores
- **PC moderado (64GB RAM, GPU 8-16GB)**: Use `Q4_K_M`
- **PC potente (128GB+ RAM, GPU 24GB+)**: Use `UD-Q2_K_XL` ou `Q5_K_M`

## ğŸ”§ MÃ©todos de ExecuÃ§Ã£o

### OpÃ§Ã£o 1: llama.cpp (Recomendado para hardware limitado)

O `llama.cpp` oferece melhor controle sobre offloading CPU/GPU e quantizaÃ§Ã£o.

**Vantagens:**
- Suporte a offloading inteligente
- Menor uso de memÃ³ria
- Melhor para hardware limitado

### OpÃ§Ã£o 2: Ollama (Mais simples)

O Ollama Ã© mais fÃ¡cil de usar, mas pode ser menos eficiente em hardware limitado.

**Vantagens:**
- InstalaÃ§Ã£o mais simples
- Interface mais amigÃ¡vel
- Gerenciamento automÃ¡tico de modelos

## ğŸ“ Exemplos de Uso

### Executar com contexto pequeno (economiza memÃ³ria)

```bash
./scripts/run-llamacpp.sh --ctx-size 4096 --threads 4
```

### Executar apenas em CPU

```bash
./scripts/run-llamacpp.sh --cpu-only
```

### Executar com offloading parcial para CPU

```bash
./scripts/run-llamacpp.sh --gpu-layers 10
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Out of memory"
- Reduza o `--ctx-size` (tamanho do contexto)
- Use uma versÃ£o mais quantizada do modelo
- Reduza `--gpu-layers` para fazer mais offload para CPU

### Erro: "CUDA not found"
- Verifique se o CUDA estÃ¡ instalado: `nvidia-smi`
- Recompile o llama.cpp com suporte CUDA

### Modelo muito lento
- Aumente `--threads` (nÃºmero de threads CPU)
- Use mais camadas na GPU se tiver VRAM disponÃ­vel
- Considere usar uma versÃ£o mais leve do modelo

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o oficial GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [Ollama Documentation](https://ollama.ai/docs)
- [Modelos quantizados no Hugging Face](https://huggingface.co/bartowski/zai-org_GLM-4.7-GGUF)

## ğŸ“„ LicenÃ§a

Este repositÃ³rio contÃ©m scripts de instalaÃ§Ã£o e configuraÃ§Ã£o. O modelo GLM-4.7 possui sua prÃ³pria licenÃ§a - consulte o repositÃ³rio oficial.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## âš ï¸ Avisos

- Modelos grandes podem demorar muito para baixar (100GB+)
- A primeira execuÃ§Ã£o pode ser lenta enquanto o modelo carrega
- Certifique-se de ter espaÃ§o em disco suficiente antes de baixar
- Em hardware muito limitado, considere usar modelos menores ou serviÃ§os em nuvem
